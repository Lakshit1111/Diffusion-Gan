{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils import spectral_norm\n\n# ------------------------ Residual Block ------------------------ #\nclass ResBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        return self.relu(out)\n\n# ------------------------ Generator ------------------------ #\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, num_features=64):\n        super().__init__()\n        self.conv1       = nn.Conv2d(in_channels, num_features, 3, padding=1)\n        self.res_blocks  = nn.Sequential(*[ResBlock(num_features) for _ in range(8)])\n        \n        # first upsample + sharpen\n        self.upconv1     = nn.ConvTranspose2d(num_features, num_features, 4, stride=2, padding=1)\n        self.res_up1     = ResBlock(num_features)\n        \n        # second upsample + sharpen\n        self.upconv2     = nn.ConvTranspose2d(num_features, num_features, 4, stride=2, padding=1)\n        self.res_up2     = ResBlock(num_features)\n        \n        # final to RGB\n        self.conv_final  = nn.Conv2d(num_features, in_channels, 3, padding=1)\n        self.tanh        = nn.Tanh()\n\n    def forward(self, x):\n        # 1) initial conv \n        feat          = self.conv1(x)\n        \n        # 2) deep residual blocks\n        out           = self.res_blocks(feat)\n        \n        # 3) upsample #1 + sharpening\n        out           = self.upconv1(out)\n        out           = self.res_up1(out)\n        \n        # 4) upsample #2 + sharpening\n        out           = self.upconv2(out)\n        out           = self.res_up2(out)\n        \n        # 5) to RGB\n        out           = self.conv_final(out)\n        return out\n\n\n\n# ------------------------ Discriminator ------------------------ #\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, base_features=64):\n        super().__init__()\n        def sn_conv(in_f, out_f, k, s, p):\n            # SpectralNorm + Conv + LeakyReLU\n            return nn.Sequential(\n                spectral_norm(nn.Conv2d(in_f, out_f, kernel_size=k, stride=s, padding=p)),\n                nn.LeakyReLU(0.2, inplace=True)\n            )\n\n        # 70×70 PatchGAN:\n        self.model = nn.Sequential(\n            # input: N×3×H×W → N×64×H/2×W/2\n            *sn_conv(in_channels,   base_features,    4, 2, 1),\n            # N×64→128, H/2→H/4\n            *sn_conv(base_features, base_features*2,  4, 2, 1),\n            # N×128→256, H/4→H/8\n            *sn_conv(base_features*2, base_features*4,4, 2, 1),\n            # N×256→512, H/8→H/16 (stride=1 to keep patch size ~70)\n            *sn_conv(base_features*4, base_features*8,4, 1, 1),\n            # Final conv to 1-channel “realness” map\n            spectral_norm(nn.Conv2d(base_features*8, 1, kernel_size=4, stride=1, padding=1))\n        )\n\n    def forward(self, x):\n        # returns N×1×H’×W’ patch map (no sigmoid)\n        return self.model(x)","metadata":{"_uuid":"14c73f99-4546-4ee5-a1d9-eed77acf4339","_cell_guid":"7b30a2cb-f098-4680-83d5-d71b7f101833","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-08T11:11:53.424714Z","iopub.execute_input":"2025-06-08T11:11:53.425406Z","iopub.status.idle":"2025-06-08T11:11:53.436358Z","shell.execute_reply.started":"2025-06-08T11:11:53.425378Z","shell.execute_reply":"2025-06-08T11:11:53.435649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchsummary import summary \n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the model\nmodel = Generator().to(device)\n\n# Dummy input - adjust size based on your expected LR image size\ndummy_input = torch.randn(1, 3, 64, 64).to(device)\n\n# Forward pass\nwith torch.no_grad():\n    output = model(dummy_input)\n\n# Print input and output shapes\nprint(f\"Input shape: {dummy_input.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\n# Optional: summary of model\n# summary(model, input_size=(3, 128, 128))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:12:11.135592Z","iopub.execute_input":"2025-06-08T11:12:11.136113Z","iopub.status.idle":"2025-06-08T11:12:11.165256Z","shell.execute_reply.started":"2025-06-08T11:12:11.136089Z","shell.execute_reply":"2025-06-08T11:12:11.164591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport random\n\nclass DIV2KDataset(Dataset):\n    def __init__(self, root_dir=\"/kaggle/input/div2k-dataset/\", train=True, scale_factor=2):\n        \"\"\"\n        Args:\n            root_dir (str): Directory with DIV2K dataset\n            train (bool): If True, creates dataset from training set, otherwise from validation set\n            scale_factor (int): Scale factor for super-resolution (default: 4x upscaling)\n        \"\"\"\n        self.root_dir = root_dir\n        self.train = train\n        self.scale_factor = scale_factor\n        \n        # Get list of image files\n        self.image_files = self._get_image_files()\n        \n        # Define transforms for LR images\n        self.lr_transform = transforms.Compose([\n            transforms.Resize(size=(128 // self.scale_factor, 128 // self.scale_factor), \n                           interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n\n    def _get_image_files(self):\n        \"\"\"Get list of image files from DIV2K dataset\"\"\"\n        if self.train:\n            image_dir = os.path.join(self.root_dir, \"DIV2K_train_HR\")\n        else:\n            image_dir = os.path.join(self.root_dir, \"DIV2K_valid_HR\")\n            \n        files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n        files.sort()  # Ensure consistent ordering\n        \n        if self.train:\n            return files[:800]  # First 800 images for training\n        return files  # All validation images\n\n    def __len__(self):\n        \"\"\"Return the total number of images in the dataset\"\"\"\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        \"\"\"Return a dictionary containing both LR and HR images\"\"\"\n        img_name = self.image_files[idx]\n        \n        # Load HR image\n        if self.train:\n            img_path = os.path.join(self.root_dir, \"DIV2K_train_HR\", img_name)\n        else:\n            img_path = os.path.join(self.root_dir, \"DIV2K_valid_HR\", img_name)\n            \n        hr_image = Image.open(img_path).convert('RGB')\n        \n        # Random crop for training\n        if self.train:\n            # Random crop to fixed size for HR\n            hr_cropped = transforms.RandomCrop(128 * self.scale_factor)(hr_image)\n            # Create LR version\n            lr_image = self.lr_transform(hr_cropped)\n            # Convert HR to tensor\n            hr_tensor = transforms.ToTensor()(hr_cropped)\n            # hr_tensor = hr_tensor * 2 - 1\n        else:\n            # For validation, use center crop\n            hr_cropped = transforms.CenterCrop(128 * self.scale_factor)(hr_image)\n            lr_image = self.lr_transform(hr_cropped)\n            hr_tensor = transforms.ToTensor()(hr_cropped)\n\n        return {\n            'lr': lr_image,  # Low resolution image\n            'hr': hr_tensor  # High resolution image\n        }\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create dataset instance\n    dataset = DIV2KDataset(root_dir=\"/kaggle/input/div2k_train_hr/\", train=True)\n    print(f\"Dataset size: {len(dataset)}\")\n    \n    # Get a sample\n    sample = dataset[0]\n    print(f\"LR image shape: {sample['lr'].shape}\")\n    print(f\"HR image shape: {sample['hr'].shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:12:22.251645Z","iopub.execute_input":"2025-06-08T11:12:22.251971Z","iopub.status.idle":"2025-06-08T11:12:25.427744Z","shell.execute_reply.started":"2025-06-08T11:12:22.251949Z","shell.execute_reply":"2025-06-08T11:12:25.427029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport os\nimport kornia\nimport kornia.losses\nimport kornia.filters\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\n\n\nclass PerceptualLoss(nn.Module):\n    def __init__(self, layers=[3, 8, 15, 22]):\n        super().__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features\n        self.selected_layers = layers\n        self.vgg = vgg.eval()\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, y):\n        loss = 0\n        for i, layer in enumerate(self.vgg):\n            x = layer(x)\n            y = layer(y)\n            if i in self.selected_layers:\n                loss += F.l1_loss(x, y)\n        return loss\n\n# Edge Loss\ndef edge_loss(pred, target):\n    pred_edges = kornia.filters.sobel(pred)\n    target_edges = kornia.filters.sobel(target)\n    return nn.L1Loss()(pred_edges, target_edges)\n\ndef tv_loss(img):\n    return torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:])) + \\\n           torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]))\n\ndef color_loss(fake, real):\n    mean_fake = fake.mean(dim=[2,3]) \n    mean_real = real.mean(dim=[2,3])\n    return nn.functional.l1_loss(mean_fake, mean_real)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:12:59.877841Z","iopub.execute_input":"2025-06-08T11:12:59.878427Z","iopub.status.idle":"2025-06-08T11:13:02.046570Z","shell.execute_reply.started":"2025-06-08T11:12:59.878404Z","shell.execute_reply":"2025-06-08T11:13:02.045835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport torchmetrics\n\ndef train(epochs=5, batch_size=8, lr=0.0002, b1=0.5, b2=0.999):\n    os.makedirs(\"results\", exist_ok=True)\n    os.makedirs(\"saved_models\", exist_ok=True)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --- Create Models and Wrap with DataParallel ---\n    generator = Generator().to(device)\n    discriminator = Discriminator().to(device)\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        generator = nn.DataParallel(generator)\n        discriminator = nn.DataParallel(discriminator)\n    else:\n        print(\"Using single GPU or CPU\")\n\n    # --- Losses and Optimizers ---\n    criterion_GAN = nn.MSELoss()\n    criterion_content = nn.L1Loss()\n    criterion_edge = edge_loss\n    criterion_perceptual = PerceptualLoss().to(device)\n\n    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\n    # --- Metrics ---\n    psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    dataset = DIV2KDataset(root_dir=\"/kaggle/input/div2k_train_hr/\", train=True)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n    generator_losses = []\n    discriminator_losses = []\n    psnr_scores = []\n    ssim_scores = []\n\n    for epoch in range(epochs):\n        g_loss_epoch = 0\n        d_loss_epoch = 0\n        psnr_epoch = 0\n        ssim_epoch = 0\n        image_shown = False\n\n        for i, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n            lr_imgs = batch['lr'].to(device)\n            hr_imgs = batch['hr'].to(device)\n            downsampled_hr_imgs = F.interpolate(hr_imgs, size=(64, 64), mode='bilinear', align_corners=False)\n\n            valid = torch.empty((lr_imgs.size(0), 1, 30, 30), device=device).uniform_(0.9, 1.0)\n            fake = torch.empty((lr_imgs.size(0), 1, 30, 30), device=device).uniform_(0.0, 0.1)\n\n            # --- Train Generator ---\n            optimizer_G.zero_grad()\n            gen_hr = generator(lr_imgs)\n            pred_fake = discriminator(gen_hr)\n\n            gen_hr_clamped = gen_hr.clamp(0, 1)\n\n            # Generator Loss\n            loss_GAN = criterion_GAN(pred_fake, valid)\n            loss_content = criterion_content(gen_hr, hr_imgs)\n            loss_perceptual = criterion_perceptual(gen_hr, hr_imgs)\n            loss_edge = criterion_edge(gen_hr, hr_imgs)\n            loss_tv = tv_loss(gen_hr)\n            loss_color = color_loss(gen_hr, hr_imgs)\n\n            loss_G = (\n                0.8 * loss_content +\n                5e-3 * loss_GAN +\n                0.05 * loss_perceptual +\n                0.05 * loss_edge +\n                0.001 * loss_tv +\n                0.01 * loss_color  \n            )\n\n            loss_G.backward()\n            optimizer_G.step()\n\n            # --- Train Discriminator ---\n            optimizer_D.zero_grad()\n            pred_real = discriminator(hr_imgs)\n            loss_real = criterion_GAN(pred_real, valid)\n\n            pred_fake = discriminator(gen_hr.detach())\n            loss_fake = criterion_GAN(pred_fake, fake)\n\n            loss_D = (loss_real + loss_fake) / 2\n            loss_D.backward()\n            optimizer_D.step()\n\n            g_loss_epoch += loss_G.item()\n            d_loss_epoch += loss_D.item()\n\n            # --- Calculate Metrics ---\n            with torch.no_grad():\n                # gen_hr_clamped = gen_hr.clamp(0, 1)\n                hr_imgs_clamped = hr_imgs.clamp(0, 1)\n\n                psnr_score = psnr_metric(gen_hr_clamped, hr_imgs_clamped)\n                ssim_score = ssim_metric(gen_hr_clamped, hr_imgs_clamped)\n\n                psnr_epoch += psnr_score.item()\n                ssim_epoch += ssim_score.item()\n\n        # Visualize Generated Images (first batch only)\n        if not image_shown:\n            num_images = min(gen_hr.size(0), 4)\n            gen_images = gen_hr[:num_images].detach().cpu()\n            real_images = hr_imgs[:num_images].detach().cpu()\n\n            fig, axes = plt.subplots(2, num_images, figsize=(12, 6))\n            for idx in range(num_images):\n                axes[0, idx].imshow(gen_images[idx].permute(1, 2, 0).clamp(0, 1))\n                axes[0, idx].set_title(\"Generated\")\n                axes[0, idx].axis(\"off\")\n\n                axes[1, idx].imshow(real_images[idx].permute(1, 2, 0).clamp(0, 1))\n                axes[1, idx].set_title(\"Real\")\n                axes[1, idx].axis(\"off\")\n\n            plt.suptitle(f\"Epoch {epoch+1}\")\n            plt.tight_layout()\n            plt.show()\n            image_shown = True\n\n        # Save Average Loss and Metrics for Epoch\n        avg_g_loss = g_loss_epoch / len(dataloader)\n        avg_d_loss = d_loss_epoch / len(dataloader)\n        avg_psnr = psnr_epoch / len(dataloader)\n        avg_ssim = ssim_epoch / len(dataloader)\n\n        generator_losses.append(avg_g_loss)\n        discriminator_losses.append(avg_d_loss)\n        psnr_scores.append(avg_psnr)\n        ssim_scores.append(avg_ssim)\n\n        print(f\"[Epoch {epoch+1}/{epochs}] Generator Loss: {avg_g_loss:.4f}, Discriminator Loss: {avg_d_loss:.4f}\")\n        print(f\"[Epoch {epoch+1}/{epochs}] PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n\n        # Save model checkpoints\n        if (epoch + 1) % 50 == 0:\n            torch.save(generator.module.state_dict(), f\"saved_models/generator_epoch_{epoch+1}.pth\")\n            torch.save(discriminator.module.state_dict(), f\"saved_models/discriminator_epoch_{epoch+1}.pth\")\n\n        torch.cuda.empty_cache()\n\n    # --- Plot Losses ---\n    plt.figure(figsize=(10, 6))\n    plt.plot(generator_losses, label=\"Generator Loss\", color='blue')\n    plt.plot(discriminator_losses, label=\"Discriminator Loss\", color='red')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Losses Over Epochs\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"results/loss_plot.png\")\n    plt.show()\n\n    # --- Plot PSNR and SSIM ---\n    plt.figure(figsize=(10, 6))\n    plt.plot(psnr_scores, label=\"PSNR\", color='green')\n    plt.plot(ssim_scores, label=\"SSIM\", color='purple')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Score\")\n    plt.title(\"Image Quality Metrics Over Epochs\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"results/metrics_plot.png\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:21:00.781444Z","iopub.execute_input":"2025-06-08T11:21:00.781710Z","iopub.status.idle":"2025-06-08T11:21:00.800282Z","shell.execute_reply.started":"2025-06-08T11:21:00.781694Z","shell.execute_reply":"2025-06-08T11:21:00.799546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrain(epochs=200, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:21:04.639555Z","iopub.execute_input":"2025-06-08T11:21:04.639795Z","iopub.status.idle":"2025-06-08T11:24:04.637780Z","shell.execute_reply.started":"2025-06-08T11:21:04.639779Z","shell.execute_reply":"2025-06-08T11:24:04.636590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T21:29:46.863310Z","iopub.status.idle":"2025-04-29T21:29:46.863577Z","shell.execute_reply.started":"2025-04-29T21:29:46.863464Z","shell.execute_reply":"2025-04-29T21:29:46.863476Z"}},"outputs":[],"execution_count":null}]}