{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-wavelets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:30:37.257992Z","iopub.execute_input":"2025-05-04T17:30:37.258287Z","iopub.status.idle":"2025-05-04T17:31:53.379546Z","shell.execute_reply.started":"2025-05-04T17:30:37.258265Z","shell.execute_reply":"2025-05-04T17:31:53.378551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:31:53.380727Z","iopub.execute_input":"2025-05-04T17:31:53.381001Z","iopub.status.idle":"2025-05-04T17:31:53.385288Z","shell.execute_reply.started":"2025-05-04T17:31:53.380979Z","shell.execute_reply":"2025-05-04T17:31:53.384615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.nn.utils import spectral_norm\nfrom pytorch_wavelets import DWTForward  # Make sure pytorch_wavelets is installed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:31:53.387395Z","iopub.execute_input":"2025-05-04T17:31:53.387867Z","iopub.status.idle":"2025-05-04T17:31:56.396232Z","shell.execute_reply.started":"2025-05-04T17:31:53.387850Z","shell.execute_reply":"2025-05-04T17:31:56.395690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add missing beta schedule function\ndef cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"\n    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clamp(betas, 0.0001, 0.9999)\n\n# ------------------------ Wavelet Block ------------------------ #\nclass WaveletBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dwt = DWTForward(J=1, mode='zero', wave='haar')\n        self.conv = nn.Conv2d(9, 64, kernel_size=1) \n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        yl, yh = self.dwt(x) \n        yh = yh[0]  # first level\n        yh = yh.reshape(yh.shape[0], -1, yh.shape[-2], yh.shape[-1]) \n\n        # Upsample high-frequency to match input\n        yh_upsampled = nn.functional.interpolate(yh, size=x.shape[-2:], mode='bilinear', align_corners=False)\n\n        features = self.relu(self.conv(yh_upsampled))\n        return features\n\n# ------------------------ Residual Block ------------------------ #\nclass ResBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        return self.relu(out)\n\n# ------------------------ Generator ------------------------ #\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, num_features=64):\n        super().__init__()\n        self.wavelet     = WaveletBlock()\n        self.conv1       = nn.Conv2d(in_channels, num_features, 3, padding=1)\n        self.res_blocks  = nn.Sequential(*[ResBlock(num_features) for _ in range(8)])\n        \n        # first upsample + sharpen\n        self.upconv1     = nn.ConvTranspose2d(num_features, num_features, 4, stride=2, padding=1)\n        self.res_up1     = ResBlock(num_features)\n        \n        # second upsample + sharpen\n        self.upconv2     = nn.ConvTranspose2d(num_features, num_features, 4, stride=2, padding=1)\n        self.res_up2     = ResBlock(num_features)\n        \n        # RGB\n        self.conv_final  = nn.Conv2d(num_features, in_channels, 3, padding=1)\n        self.tanh        = nn.Tanh()\n\n    def forward(self, x):\n        # 1) initial conv + wavelet\n        feat          = self.conv1(x)\n        wavelet_feats = self.wavelet(x)\n        out           = feat + wavelet_feats\n        \n        # 2) deep residual blocks\n        out = self.res_blocks(out)\n        \n        # 3) upsample + sharpening\n        out = self.upconv1(out)\n        out = self.res_up1(out)\n        \n        # 4) upsample + sharpening\n        out = self.upconv2(out)\n        out = self.res_up2(out)\n        \n        # 5) to RGB\n        out = self.conv_final(out)\n        return out\n\n# ------------------------ Discriminator ------------------------ #\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, base_features=64):\n        super().__init__()\n        def sn_conv(in_f, out_f, k, s, p):\n            # SpectralNorm + Conv + LeakyReLU\n            return nn.Sequential(\n                spectral_norm(nn.Conv2d(in_f, out_f, kernel_size=k, stride=s, padding=p)),\n                nn.LeakyReLU(0.2, inplace=True)\n            )\n\n        # 70×70 PatchGAN:\n        self.model = nn.Sequential(\n            # input: N×3×H×W → N×64×H/2×W/2\n            *sn_conv(in_channels,   base_features,    4, 2, 1),\n            # N×64→128, H/2→H/4\n            *sn_conv(base_features, base_features*2,  4, 2, 1),\n            # N×128→256, H/4→H/8\n            *sn_conv(base_features*2, base_features*4,4, 2, 1),\n            # N×256→512, H/8→H/16 (stride=1 to keep patch size ~70)\n            *sn_conv(base_features*4, base_features*8,4, 1, 1),\n            # Final conv to 1-channel \"realness\" map\n            spectral_norm(nn.Conv2d(base_features*8, 1, kernel_size=4, stride=1, padding=1))\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# ------------------------ UNet Denoiser ------------------------ #\n# --- Timestep Embedding --- #\nclass SinusoidalTimeEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        half_dim = self.dim // 2\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -(math.log(10000) / (half_dim)))\n        emb = t[:, None].float() * emb[None, :]\n        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n# --- Self-Attention Block --- #\nclass SelfAttention(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.key   = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.value = nn.Conv2d(in_dim, in_dim, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        q = self.query(x).view(B, -1, H*W).permute(0, 2, 1)\n        k = self.key(x).view(B, -1, H*W)                    \n        attn = torch.bmm(q, k) / (C ** 0.5)                \n        attn = torch.softmax(attn, dim=-1)\n\n        v = self.value(x).view(B, -1, H*W)             \n        out = torch.bmm(v, attn.permute(0, 2, 1)).view(B, C, H, W)\n        return self.gamma * out + x\n\n# --- UNet Block with Time Embedding --- #\nclass UNetBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, time_emb_dim=None):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.InstanceNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.InstanceNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n        if time_emb_dim:\n            self.time_proj = nn.Sequential(\n                nn.Linear(time_emb_dim, out_ch),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.time_proj = None\n\n    def forward(self, x, t_emb=None):\n        out = self.conv(x)\n        if self.time_proj is not None and t_emb is not None:\n            B, C, H, W = out.shape\n            time_feat = self.time_proj(t_emb).view(B, C, 1, 1)\n            out = out + time_feat\n        return out\n\n# --- Full UNet Denoiser --- #\nclass UNetDenoiser(nn.Module):\n    def __init__(self, in_channels=3, base=64, time_emb_dim=128):\n        super().__init__()\n        self.time_embed = SinusoidalTimeEmbedding(time_emb_dim)\n\n        self.enc1 = UNetBlock(in_channels, base, time_emb_dim)\n        self.enc2 = UNetBlock(base, base*2, time_emb_dim)\n        self.enc3 = UNetBlock(base*2, base*4, time_emb_dim)\n        self.enc4 = UNetBlock(base*4, base*8, time_emb_dim)\n\n        self.pool = nn.MaxPool2d(2)\n\n        self.middle = UNetBlock(base*8, base*8, time_emb_dim)\n        self.attention =  SelfAttention(base*8)\n\n        self.up4 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n        self.dec4 = UNetBlock(base*12, base*4, time_emb_dim)\n\n        self.up3 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n        self.dec3 = UNetBlock(base*6, base*2, time_emb_dim)\n\n        self.up2 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n        self.dec2 = UNetBlock(base*3, base, time_emb_dim)\n\n        self.up1 = nn.ConvTranspose2d(base, base, 2, stride=2)\n        self.dec1 = UNetBlock(base*2, base, time_emb_dim)\n\n        self.outc = nn.Conv2d(base, in_channels, 1)\n\n    def forward(self, x, t):\n        t_emb = self.time_embed(t)\n\n        e1 = self.enc1(x, t_emb)\n        e2 = self.enc2(self.pool(e1), t_emb)\n        e3 = self.enc3(self.pool(e2), t_emb)\n        e4 = self.enc4(self.pool(e3), t_emb)\n\n        m = self.middle(self.pool(e4), t_emb)\n        m = self.attention(m)\n\n        d4 = self.up4(m)\n        d4 = self.dec4(torch.cat([d4, e4], dim=1), t_emb)\n\n        d3 = self.up3(d4)\n        d3 = self.dec3(torch.cat([d3, e3], dim=1), t_emb)\n\n        d2 = self.up2(d3)\n        d2 = self.dec2(torch.cat([d2, e2], dim=1), t_emb)\n\n        d1 = self.up1(d2)\n        d1 = self.dec1(torch.cat([d1, e1], dim=1), t_emb)\n\n        return self.outc(d1)\n\n# ------------------------ Improved Diffusion Generator ------------------------ #\nclass DiffusionGenerator(nn.Module):\n    def __init__(self, in_channels=3, timesteps=1000):\n        super().__init__()\n        self.timesteps = timesteps\n        self.denoiser = UNetDenoiser(in_channels)\n        self.register_buffer(\"betas\", cosine_beta_schedule(timesteps))\n        \n        # Pre-compute important values used during training and sampling\n        alphas = 1. - self.betas\n        self.register_buffer(\"alphas_cumprod\", torch.cumprod(alphas, dim=0))\n        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(self.alphas_cumprod))\n        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1. - self.alphas_cumprod))\n        \n        # For sampling\n        self.register_buffer(\"sqrt_recip_alphas\", torch.sqrt(1.0 / alphas))\n        posterior_variance = self.betas * (1. - self.alphas_cumprod.clone() / self.alphas_cumprod)\n        self.register_buffer(\"posterior_variance\", posterior_variance)\n        self.register_buffer(\"posterior_log_variance_clipped\", torch.log(posterior_variance.clamp(min=1e-20)))\n        self.register_buffer(\"posterior_mean_coef1\", self.betas * torch.sqrt(self.alphas_cumprod) / (1. - self.alphas_cumprod))\n        self.register_buffer(\"posterior_mean_coef2\", (1. - self.alphas_cumprod) * torch.sqrt(alphas) / (1. - self.alphas_cumprod))\n\n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"Forward diffusion: add noise to the image\"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n            \n        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n        \n        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise, noise\n\n    def p_losses(self, x_start, t, noise=None):\n        \"\"\"Training loss for the denoiser\"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n            \n        x_noisy, _ = self.q_sample(x_start, t, noise)\n        predicted_noise = self.denoiser(x_noisy, t)\n        \n        loss = F.mse_loss(predicted_noise, noise)\n        return loss, x_noisy\n\n    @torch.no_grad()\n    def p_sample(self, x, t):\n        \"\"\"Sample from the model at timestep t\"\"\"\n        betas_t = self.betas[t].view(-1, 1, 1, 1)\n        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n        sqrt_recip_alphas_t = self.sqrt_recip_alphas[t].view(-1, 1, 1, 1)\n        \n        # Predict the noise\n        model_output = self.denoiser(x, t)\n        \n        # Get the predicted x_0\n        pred_original_sample = (x - sqrt_one_minus_alphas_cumprod_t * model_output) * sqrt_recip_alphas_t\n        \n        # Add noise for t > 0\n        if t[0] > 0:\n            noise = torch.randn_like(x)\n            variance = torch.sqrt(self.posterior_variance[t].view(-1, 1, 1, 1))\n            x = pred_original_sample + variance * noise\n            \n        return x\n\n    @torch.no_grad()\n    def sample(self, x, steps=100):\n        \"\"\"Gradually remove noise from a noisy image\"\"\"\n        # Use a subset of timesteps for faster sampling\n        timesteps = torch.linspace(0, self.timesteps - 1, steps).long().to(x.device)\n        timesteps = timesteps.flip(0)  # Reverse for denoising\n        \n        curr_x = x\n        \n        for i, t in enumerate(timesteps):\n            t_batch = torch.full((curr_x.shape[0],), t, device=curr_x.device, dtype=torch.long)\n            curr_x = self.p_sample(curr_x, t_batch)\n            \n        return curr_x\n            \n    def forward(self, x, sampling_steps=100):\n        \"\"\"Add noise and then denoise\"\"\"\n        # Add noise according to the diffusion process\n        t = torch.randint(0, self.timesteps, (x.size(0),), device=x.device)\n        x_noisy, _ = self.q_sample(x, t)\n        \n        # Then denoise to get diffusion-processed image\n        x_denoised = self.sample(x_noisy, steps=sampling_steps)\n        \n        return x_denoised\n\n# ------------------------ Combined Generator ------------------------ #\nclass CombinedGenerator(nn.Module):\n    def __init__(self, in_channels=3, diffusion_steps=100):\n        super().__init__()\n        self.diffusion = DiffusionGenerator(in_channels, timesteps=1000)\n        self.gan = Generator(in_channels)\n        self.diffusion_steps = diffusion_steps\n\n    def forward(self, x):\n        # Use diffusion model to enhance details\n        x_diff = self.diffusion(x, sampling_steps=self.diffusion_steps)\n        # Then use GAN to super-resolve\n        gen_hr = self.gan(x_diff)\n        return gen_hr, x_diff","metadata":{"_uuid":"14c73f99-4546-4ee5-a1d9-eed77acf4339","_cell_guid":"7b30a2cb-f098-4680-83d5-d71b7f101833","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T17:31:56.396981Z","iopub.execute_input":"2025-05-04T17:31:56.397355Z","iopub.status.idle":"2025-05-04T17:31:56.434775Z","shell.execute_reply.started":"2025-05-04T17:31:56.397335Z","shell.execute_reply":"2025-05-04T17:31:56.434128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchsummary import summary  # Optional, for model summary\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the model\nmodel = CombinedGenerator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Dummy input - adjust size based on your expected LR image size\ndummy_input = torch.randn(1, 3, 64, 64).to(device)\n\n# Forward pass\nwith torch.no_grad():\n    output = model(dummy_input)\n    d_out = discriminator(output[0])\n\n# Print input and output shapes\nprint(f\"Input shape: {dummy_input.shape}\")\nprint(f\"Output shape: {output[0].shape}\")\nprint(f\"Output diffusion shape: {output[1].shape}\")\nprint(f\"Output disscriminator shape: {d_out.shape}\")\n\n# Optional: summary of model\n# summary(model, input_size=(3, 128, 128))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:31:56.435553Z","iopub.execute_input":"2025-05-04T17:31:56.435798Z","iopub.status.idle":"2025-05-04T17:32:01.536963Z","shell.execute_reply.started":"2025-05-04T17:31:56.435771Z","shell.execute_reply":"2025-05-04T17:32:01.536194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport random\nimport numpy as np\nimport math\n\nclass DIV2KDataset(Dataset):\n    \"\"\"DIV2K dataset with HR images and synthetic LR pairs\"\"\"\n    def __init__(self, root_dir, train=True, scale_factor=4, patch_size=256, augment=True):\n        \"\"\"\n        Args:\n            root_dir: Directory with HR images\n            train: Whether to use training or validation set\n            scale_factor: Downsampling factor for LR images\n            patch_size: Size of HR patches to extract\n            augment: Whether to apply data augmentation\n        \"\"\"\n        self.root_dir = root_dir\n        self.patch_size = patch_size\n        self.scale_factor = scale_factor\n        self.augment = augment\n        self.lr_patch_size = patch_size // scale_factor\n        \n        # Get file list\n        split = 'train' if train else 'val'\n        self.hr_files = sorted([os.path.join(root_dir, f) for f in os.listdir(root_dir) \n                                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')) \n                                ])\n        \n        # Transformations for augmentation\n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(90)\n        ])\n        \n        # Create a unique directory for temp files if needed\n        self.temp_dir = os.path.join(os.getcwd(), \"temp_compression_files\")\n        os.makedirs(self.temp_dir, exist_ok=True)\n    \n    def __len__(self):\n        return len(self.hr_files)\n    \n    def __getitem__(self, idx):\n        try:\n            # Load HR image\n            hr_img = Image.open(self.hr_files[idx]).convert('RGB')\n            \n            # Random crop if image is large enough\n            if hr_img.width > self.patch_size and hr_img.height > self.patch_size:\n                # Random crop\n                left = random.randint(0, hr_img.width - self.patch_size)\n                top = random.randint(0, hr_img.height - self.patch_size)\n                hr_img = hr_img.crop((left, top, left + self.patch_size, top + self.patch_size))\n            else:\n                # Resize if image is too small\n                hr_img = hr_img.resize((self.patch_size, self.patch_size), Image.BICUBIC)\n            \n            # Apply augmentation\n            if self.augment:\n                hr_img = self.transform(hr_img)\n            \n            # Convert to tensor and normalize\n            hr_tensor = transforms.ToTensor()(hr_img)\n            \n            # Generate LR image with bicubic downsampling\n            lr_tensor = torch.nn.functional.interpolate(\n                hr_tensor.unsqueeze(0), \n                scale_factor=1/self.scale_factor, \n                mode='bicubic', \n                align_corners=False\n            ).squeeze(0)\n            \n            # Add Gaussian noise to simulate real LR images\n            noise = torch.randn_like(lr_tensor) * 0.01\n            lr_tensor = torch.clamp(lr_tensor + noise, 0, 1)\n            \n            # JPEG compression artifacts\n            if random.random() < 0.5:\n                # Create a unique temp file path for this worker and index\n                # Use thread/process ID to avoid conflicts between DataLoader workers\n                pid = os.getpid()\n                temp_file = os.path.join(self.temp_dir, f\"temp_{pid}_{idx}.jpg\")\n                \n                try:\n                    # Convert to PIL image with quality degradation\n                    lr_img = transforms.ToPILImage()(lr_tensor)\n                    # Save with JPEG compression\n                    compression_factor = random.randint(60, 95)\n                    lr_img.save(temp_file, quality=compression_factor)\n                    \n                    # Reload as tensor if file exists\n                    if os.path.exists(temp_file):\n                        lr_tensor = transforms.ToTensor()(Image.open(temp_file))\n                        # Remove temporary file\n                        os.remove(temp_file)\n                except Exception as e:\n                    # If any error occurs, log it and continue with the original tensor\n                    print(f\"Warning: JPEG compression failed for idx {idx}: {e}\")\n            \n            return {'lr': lr_tensor, 'hr': hr_tensor}\n            \n        except Exception as e:\n            print(f\"Error processing sample {idx} from {self.hr_files[idx]}: {e}\")\n            # Return a placeholder if processing fails\n            # This allows the dataloader to continue even if one sample fails\n            placeholder_hr = torch.zeros(3, self.patch_size, self.patch_size)\n            placeholder_lr = torch.zeros(3, self.lr_patch_size, self.lr_patch_size)\n            return {'lr': placeholder_lr, 'hr': placeholder_hr}\n    \n    def __del__(self):\n        \"\"\"Clean up any remaining temporary files when the dataset is destroyed\"\"\"\n        try:\n            # Remove the temp directory and all its contents if it exists\n            if os.path.exists(self.temp_dir):\n                for file in os.listdir(self.temp_dir):\n                    try:\n                        os.remove(os.path.join(self.temp_dir, file))\n                    except:\n                        pass\n                os.rmdir(self.temp_dir)\n        except:\n            pass\n            \ndef setup_diffusion_eval_grid(generator, device, num_examples=4):\n    \"\"\"Creates a visualization grid to show diffusion denoising process\"\"\"\n    # Get some evaluation samples \n    dataset = DIV2KDataset(root_dir=\"/kaggle/input/div2k_train_hr/\", train=False)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=num_examples, shuffle=True)\n    \n    lr_imgs, hr_imgs = next(iter(dataloader))['lr'].to(device), next(iter(dataloader))['hr'].to(device)\n    \n    # Extract diffusion component\n    diffusion = generator.module.diffusion if isinstance(generator, torch.nn.DataParallel) else generator.diffusion\n    timesteps = diffusion.timesteps\n    \n    # Save original LR images \n    images = [lr_imgs.cpu()]\n    \n    # Add noise progressively to show forward process\n    t_steps = [int(timesteps * p) for p in [0.25, 0.5, 0.75, 1.0]]\n    for t in t_steps:\n        t_batch = torch.tensor([t] * lr_imgs.shape[0], device=device)\n        noisy_imgs, _ = diffusion.q_sample(lr_imgs, t_batch)\n        images.append(noisy_imgs.cpu())\n    \n    # Denoise progressively to show reverse process\n    x = noisy_imgs\n    reverse_steps = [int(timesteps * p) for p in [0.75, 0.5, 0.25, 0.0]]\n    for t in reverse_steps:\n        t_batch = torch.tensor([t] * x.shape[0], device=device)\n        with torch.no_grad():\n            x = diffusion.p_sample(x, t_batch)\n        images.append(x.cpu())\n        \n    # Final images (LR, HR, SR)\n    with torch.no_grad():\n        gen_hr, _ = generator(lr_imgs)\n    images.append(gen_hr.cpu())  # SR output\n    images.append(hr_imgs.cpu())  # Ground truth HR\n    \n    # Create a grid\n    rows = len(images)\n    cols = num_examples\n    grid = torch.zeros(rows * cols, 3, lr_imgs.shape[2], lr_imgs.shape[3])\n    \n    for i, batch in enumerate(images):\n        for j in range(cols):\n            grid[i*cols + j] = batch[j]\n            \n    return grid.clamp(0, 1)\n\ndef evaluate_model(generator, discriminator, test_dataloader, device, num_samples=5):\n    \"\"\"Evaluate model on test set and print metrics\"\"\"\n    generator.eval()\n    discriminator.eval()\n    \n    psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n    \n    total_psnr = 0\n    total_ssim = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for i, batch in enumerate(test_dataloader):\n            if i >= num_samples:\n                break\n                \n            lr_imgs = batch['lr'].to(device)\n            hr_imgs = batch['hr'].to(device)\n            \n            # Generate SR images\n            sr_imgs, _ = generator(lr_imgs)\n            sr_imgs = sr_imgs.clamp(0, 1)\n            \n            # Calculate metrics\n            psnr = psnr_metric(sr_imgs, hr_imgs)\n            ssim = ssim_metric(sr_imgs, hr_imgs)\n            \n            total_psnr += psnr.item() * lr_imgs.size(0)\n            total_ssim += ssim.item() * lr_imgs.size(0)\n            total_samples += lr_imgs.size(0)\n            \n            # Save sample images\n            if i == 0:\n                # Create comparison grid\n                grid_imgs = []\n                for j in range(min(4, lr_imgs.size(0))):\n                    # Bicubic upscale for reference\n                    bicubic = torch.nn.functional.interpolate(\n                        lr_imgs[j:j+1], scale_factor=4, mode='bicubic', align_corners=False\n                    )\n                    grid_imgs.extend([\n                        bicubic.squeeze(0),\n                        sr_imgs[j],\n                        hr_imgs[j]\n                    ])\n                \n                # Save grid\n                torchvision.utils.save_image(\n                    grid_imgs, \n                    f\"results/test_samples.png\", \n                    nrow=3, \n                    normalize=True\n                )\n    \n    # Calculate average metrics\n    avg_psnr = total_psnr / total_samples\n    avg_ssim = total_ssim / total_samples\n    \n    print(f\"Test Results: PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n    return avg_psnr, avg_ssim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:40:03.664906Z","iopub.execute_input":"2025-05-04T17:40:03.665874Z","iopub.status.idle":"2025-05-04T17:40:03.688802Z","shell.execute_reply.started":"2025-05-04T17:40:03.665836Z","shell.execute_reply":"2025-05-04T17:40:03.688084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(os.listdir(\"/kaggle/input/div2k_train_hr/DIV2K_train_HR\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:32:04.250553Z","iopub.execute_input":"2025-05-04T17:32:04.250933Z","iopub.status.idle":"2025-05-04T17:32:04.277722Z","shell.execute_reply.started":"2025-05-04T17:32:04.250914Z","shell.execute_reply":"2025-05-04T17:32:04.277132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Create dataset instance\n    dataset = DIV2KDataset(root_dir=\"/kaggle/input/DIV2K_train_HR/DIV2K_train_HR\", train=True)\n    print(f\"Dataset size: {len(dataset)}\")\n    \n    # Get a sample\n    sample = dataset[0]\n    print(f\"LR image shape: {sample['lr'].shape}\")\n    print(f\"HR image shape: {sample['hr'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:40:17.057610Z","iopub.execute_input":"2025-05-04T17:40:17.057879Z","iopub.status.idle":"2025-05-04T17:40:17.194548Z","shell.execute_reply.started":"2025-05-04T17:40:17.057860Z","shell.execute_reply":"2025-05-04T17:40:17.193649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport os\nimport pytorch_wavelets\nimport kornia\nfrom pytorch_wavelets import DWTForward\nimport kornia.losses\nimport kornia.filters\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\n\n\nclass PerceptualLoss(nn.Module):\n    def __init__(self, layers=[3, 8, 15, 22]):\n        super().__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features\n        self.selected_layers = layers\n        self.vgg = vgg.eval()\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, y):\n        loss = 0\n        for i, layer in enumerate(self.vgg):\n            x = layer(x)\n            y = layer(y)\n            if i in self.selected_layers:\n                loss += F.l1_loss(x, y)\n        return loss\n\ndef edge_loss(pred, target):\n    pred_edges = kornia.filters.sobel(pred)\n    target_edges = kornia.filters.sobel(target)\n    return nn.L1Loss()(pred_edges, target_edges)\n\n\nclass WaveletLoss(nn.Module):\n    def __init__(self):\n        super(WaveletLoss, self).__init__()\n        self.dwt = DWTForward(J=1, mode='zero', wave='haar')\n\n    def forward(self, pred, target):\n        yl_pred, yh_pred = self.dwt(pred)\n        yl_target, yh_target = self.dwt(target)\n\n        loss_l = nn.L1Loss()(yl_pred, yl_target)\n        loss_h = nn.L1Loss()(yh_pred[0], yh_target[0])\n        return loss_l + loss_h\n\ndef tv_loss(img):\n    return torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:])) + \\\n           torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]))\n\ndef color_loss(fake, real):\n    mean_fake = fake.mean(dim=[2,3)\n    mean_real = real.mean(dim=[2,3])\n    return nn.functional.l1_loss(mean_fake, mean_real)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:32:04.483065Z","iopub.execute_input":"2025-05-04T17:32:04.483340Z","iopub.status.idle":"2025-05-04T17:32:06.478308Z","shell.execute_reply.started":"2025-05-04T17:32:04.483322Z","shell.execute_reply":"2025-05-04T17:32:06.477778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torchmetrics\nfrom pytorch_wavelets import DWTForward\nimport contextlib\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:32:06.478930Z","iopub.execute_input":"2025-05-04T17:32:06.479261Z","iopub.status.idle":"2025-05-04T17:32:09.776788Z","shell.execute_reply.started":"2025-05-04T17:32:06.479242Z","shell.execute_reply":"2025-05-04T17:32:09.776230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tv_loss(x):\n    \"\"\"Total variation loss for image smoothness\"\"\"\n    batch_size = x.size()[0]\n    h_x = x.size()[2]\n    w_x = x.size()[3]\n    count_h = (x.size()[2] - 1) * x.size()[3]\n    count_w = x.size()[2] * (x.size()[3] - 1)\n    h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x-1, :]), 2).sum()\n    w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x-1]), 2).sum()\n    return (h_tv + w_tv) / (batch_size * 3 * count_h * count_w)\n\ndef edge_loss(pred, target, alpha=1.0):\n    \"\"\"Edge preservation loss using Sobel operator\"\"\"\n    def sobel(x):\n        # Define sobel filters\n        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=x.device).view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=x.device).view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n        \n        # Apply sobel filters to each channel (RGB)\n        grad_x = F.conv2d(x, sobel_x, padding=1, groups=3)\n        grad_y = F.conv2d(x, sobel_y, padding=1, groups=3)\n        \n        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-8)\n    \n    # Get edges\n    edges_pred = sobel(pred)\n    edges_target = sobel(target)\n    \n    # MSE between edges\n    return F.mse_loss(edges_pred, edges_target) * alpha\n\ndef color_loss(pred, target, alpha=1.0):\n    \"\"\"Color fidelity loss by comparing RGB channels\"\"\"\n    # Calculate mean color per channel\n    pred_mean = pred.mean(dim=[2, 3])\n    target_mean = target.mean(dim=[2, 3])\n    \n    # Calculate color loss\n    return F.mse_loss(pred_mean, target_mean) * alpha\n\nclass WaveletLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dwt = DWTForward(J=3, mode='zero', wave='haar')\n        self.criterion = nn.L1Loss()\n        \n    def forward(self, x, y):\n        \"\"\"Compare wavelet coefficients at multiple levels\"\"\"\n        # Get wavelet decomposition\n        x_ll, x_h = self.dwt(x)\n        y_ll, y_h = self.dwt(y)\n        \n        # Compare low frequency coefficients\n        loss = self.criterion(x_ll, y_ll)\n        \n        # Compare high frequency coefficients at different levels\n        for i in range(len(x_h)):\n            loss += self.criterion(x_h[i], y_h[i])\n            \n        return loss\n\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use VGG features for perceptual loss\n        vgg = torchvision.models.vgg19(pretrained=True).features\n        self.slices = nn.Sequential()\n        for i in range(16):  # Use layers up to relu4_1\n            self.slices.add_module(str(i), vgg[i])\n            \n        # Freeze parameters\n        for param in self.slices.parameters():\n            param.requires_grad = False\n            \n        self.criterion = nn.L1Loss()\n        \n    def forward(self, x, y):\n        x = (x - 0.5) * 2\n        y = (y - 0.5) * 2\n        \n        x_features = self.slices(x)\n        y_features = self.slices(y)\n        \n        return self.criterion(x_features, y_features)\n\ndef train(\n    epochs=50,            \n    batch_size=8,\n    lr=0.0001,             \n    b1=0.5,\n    b2=0.999,\n    diffusion_steps=100,   \n    diffusion_weight=0.2, \n    save_interval=5        \n):\n    os.makedirs(\"results\", exist_ok=True)\n    os.makedirs(\"saved_models\", exist_ok=True)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # --- Add this safety check for CUDA memory ---\n    if torch.cuda.is_available():\n        # Try setting to deterministic mode for better debugging\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \n        # Check available GPU memory before starting\n        for i in range(torch.cuda.device_count()):\n            total_mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}, Total memory: {total_mem:.2f} GB\")\n    \n    # Force garbage collection to start clean\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        \n\n    # --- Create Models ---\n    generator = CombinedGenerator(in_channels=3, diffusion_steps=diffusion_steps).to(device)\n    discriminator = Discriminator().to(device)\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        # Use only first two GPUs if available to avoid memory issues\n        device_ids = list(range(min(2, torch.cuda.device_count())))\n        print(f\"Limiting to {len(device_ids)} GPUs: {device_ids}\")\n        \n        generator = nn.DataParallel(generator, device_ids=device_ids)\n        discriminator = nn.DataParallel(discriminator, device_ids=device_ids)\n    else:\n        print(\"Using single GPU or CPU\")\n\n    # --- Losses and Optimizers ---\n    criterion_GAN = nn.BCEWithLogitsLoss()\n    criterion_content = nn.L1Loss()\n    criterion_diffusion = nn.MSELoss()\n    criterion_perceptual = PerceptualLoss().to(device)\n    criterion_edge = edge_loss\n    criterion_wavelet = WaveletLoss().to(device)\n\n    # Separate optimizers for diffusion and GAN components\n    diffusion_params = generator.module.diffusion.parameters() if isinstance(generator, nn.DataParallel) else generator.diffusion.parameters()\n    gan_params = generator.module.gan.parameters() if isinstance(generator, nn.DataParallel) else generator.gan.parameters()\n    \n    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n    optimizer_Diff = torch.optim.Adam(diffusion_params, lr=lr/2, betas=(b1, b2))\n    optimizer_G = torch.optim.Adam(gan_params, lr=lr, betas=(b1, b2))\n\n    # --- Learning Rate Schedulers ---\n    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=20, gamma=0.5)\n    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=20, gamma=0.5)\n    scheduler_Diff = torch.optim.lr_scheduler.StepLR(optimizer_Diff, step_size=20, gamma=0.5)\n\n    # --- Metrics ---\n    psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    dataset = DIV2KDataset(root_dir=\"/kaggle/input/DIV2K_train_HR/DIV2K_train_HR\", train=True)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n    generator_losses = []\n    discriminator_losses = []\n    diffusion_losses = []\n    psnr_scores = []\n    ssim_scores = []\n\n    for epoch in range(epochs):\n        g_loss_epoch = 0\n        d_loss_epoch = 0\n        diff_loss_epoch = 0\n        psnr_epoch = 0\n        ssim_epoch = 0\n        \n        # Tqdm progress bar\n        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), \n                           desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n\n        for i, batch in progress_bar:\n            lr_imgs = batch['lr'].to(device)\n            hr_imgs = batch['hr'].to(device)\n\n            valid = torch.ones((lr_imgs.size(0), 1, 30, 30), device=device)\n            fake = torch.zeros((lr_imgs.size(0), 1, 30, 30), device=device)\n\n            # --- Train Diffusion Denoiser ---\n            optimizer_Diff.zero_grad()\n            \n            # Random timesteps for diffusion training\n            t = torch.randint(0, generator.module.diffusion.timesteps if isinstance(generator, nn.DataParallel) \n                             else generator.diffusion.timesteps, \n                             (lr_imgs.size(0),), device=device)\n            \n            # Get diffusion loss directly from the model\n            diff_loss, _ = generator.module.diffusion.p_losses(lr_imgs, t) if isinstance(generator, nn.DataParallel) \\\n                else generator.diffusion.p_losses(lr_imgs, t)\n            \n            diff_loss.backward()\n            optimizer_Diff.step()\n            diff_loss_epoch += diff_loss.item()\n\n            # --- Train Generator ---\n            optimizer_G.zero_grad()\n            \n            # Forward pass through the combined model\n            gen_hr, x_diff = generator(lr_imgs)\n            \n            # Clamp outputs to valid range\n            gen_hr = gen_hr.clamp(0, 1)\n            x_diff = x_diff.clamp(0, 1)\n            \n            # Discriminator outputs for fake content\n            pred_fake = discriminator(gen_hr)\n\n            # Calculate generator loss components\n            loss_GAN = criterion_GAN(pred_fake, valid)\n            loss_content = criterion_content(gen_hr, hr_imgs)\n            loss_diff_mse = criterion_content(x_diff, lr_imgs)  # Ensure diffusion output resembles input\n            loss_perceptual = criterion_perceptual(gen_hr, hr_imgs)\n            loss_edge = criterion_edge(gen_hr, hr_imgs)\n            loss_wavelet = criterion_wavelet(gen_hr, hr_imgs)\n            loss_tv = tv_loss(gen_hr)\n            loss_color = color_loss(gen_hr, hr_imgs)\n\n            # Combined loss with rebalanced weights\n            loss_G = (\n                0.5 * loss_content +      # Reduced slightly\n                0.1 * loss_GAN +          # Increased importance\n                0.15 * loss_perceptual +  # Increased perceptual weight\n                0.05 * loss_edge +\n                0.05 * loss_wavelet +\n                0.001 * loss_tv +\n                0.02 * loss_color         # Slightly more color fidelity\n            )\n\n            # Backprop\n            loss_G.backward()\n            optimizer_G.step()\n\n            # --- Train Discriminator ---\n            optimizer_D.zero_grad()\n            \n            # Real images\n            pred_real = discriminator(hr_imgs)\n            loss_real = criterion_GAN(pred_real, valid)\n            \n            # Fake images\n            pred_fake = discriminator(gen_hr.detach())  # detach to avoid updating generator\n            loss_fake = criterion_GAN(pred_fake, fake)\n            \n            # Combined discriminator loss\n            loss_D = (loss_real + loss_fake) / 2\n            \n            # Backprop\n            loss_D.backward()\n            optimizer_D.step()\n\n            # Track losses\n            g_loss_epoch += loss_G.item()\n            d_loss_epoch += loss_D.item()\n\n            # --- Calculate Metrics ---\n            with torch.no_grad():\n                gen_hr_clamped = gen_hr.clamp(0, 1)\n                hr_imgs_clamped = hr_imgs.clamp(0, 1)\n\n                psnr_score = psnr_metric(gen_hr_clamped, hr_imgs_clamped)\n                ssim_score = ssim_metric(gen_hr_clamped, hr_imgs_clamped)\n\n                psnr_epoch += psnr_score.item()\n                ssim_epoch += ssim_score.item()\n                \n            # Update progress bar\n            progress_bar.set_postfix({\n                'G_loss': loss_G.item(), \n                'D_loss': loss_D.item(), \n                'Diff_loss': diff_loss.item(),\n                'PSNR': psnr_score.item(),\n                'SSIM': ssim_score.item()\n            })\n            \n        # End of batch loop\n\n        # --- Update Learning Rates ---\n        scheduler_G.step()\n        scheduler_D.step()\n        scheduler_Diff.step()\n\n        # Calculate average metrics for epoch\n        avg_g_loss = g_loss_epoch / len(dataloader)\n        avg_d_loss = d_loss_epoch / len(dataloader)\n        avg_diff_loss = diff_loss_epoch / len(dataloader)\n        avg_psnr = psnr_epoch / len(dataloader)\n        avg_ssim = ssim_epoch / len(dataloader)\n\n        # Record metrics\n        generator_losses.append(avg_g_loss)\n        discriminator_losses.append(avg_d_loss)\n        diffusion_losses.append(avg_diff_loss)\n        psnr_scores.append(avg_psnr)\n        ssim_scores.append(avg_ssim)\n\n        # Print epoch summary\n        print(f\"[Epoch {epoch+1}/{epochs}] Generator: {avg_g_loss:.4f}, Discriminator: {avg_d_loss:.4f}, Diffusion: {avg_diff_loss:.4f}\")\n        print(f\"[Epoch {epoch+1}/{epochs}] PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n\n        # Visualize samples at the end of each epoch\n        if (epoch + 1) % 1 == 0:\n            # Select a small batch for visualization\n            with torch.no_grad():\n                try:\n                    num_images = min(gen_hr.size(0), 4)\n                    gen_images = gen_hr[:num_images].detach().cpu()\n                    real_images = hr_imgs[:num_images].detach().cpu()\n        \n                    fig, axes = plt.subplots(2, num_images, figsize=(12, 6))\n                    for idx in range(num_images):\n                        axes[0, idx].imshow(gen_images[idx].permute(1, 2, 0).clamp(0, 1))\n                        axes[0, idx].set_title(\"Generated\")\n                        axes[0, idx].axis(\"off\")\n        \n                        axes[1, idx].imshow(real_images[idx].permute(1, 2, 0).clamp(0, 1))\n                        axes[1, idx].set_title(\"Real\")\n                        axes[1, idx].axis(\"off\")\n        \n                    plt.suptitle(f\"Epoch {epoch+1}\")\n                    plt.tight_layout()\n                    plt.show()\n                    image_shown = True\n                except Exception as e:\n                    print(f\"Visualization error: {e}\")\n                    print(\"Skipping visualization for this epoch\")\n                    \n                    # Make sure to restore the generator to its original state\n                    if isinstance(generator, nn.DataParallel):\n                        # Make sure module is on the right device\n                        generator.module.to(device)\n                    else:\n                        generator = generator.to(device)\n                    \n                    # Force garbage collection\n                    gc.collect()\n                    torch.cuda.empty_cache()\n    \n    # --- Additional safety measures for model saving ---\n    if (epoch + 1) % save_interval == 0:\n        try:\n            # Move models to CPU for safer saving\n            if isinstance(generator, nn.DataParallel):\n                gen_state = generator.module.state_dict()\n            else:\n                gen_state = generator.state_dict()\n                \n            if isinstance(discriminator, nn.DataParallel):\n                disc_state = discriminator.module.state_dict()\n            else:\n                disc_state = discriminator.state_dict()\n            \n            # Save models\n            torch.save(gen_state, f\"saved_models/generator_epoch_{epoch+1}.pth\")\n            torch.save(disc_state, f\"saved_models/discriminator_epoch_{epoch+1}.pth\")\n            \n        except Exception as e:\n            print(f\"Error saving models: {e}\")\n            print(\"Attempting emergency save...\")\n            torch.save(gen_state, f\"saved_models/generator_emergency.pth\")\n            \n        # Clear CUDA cache\n        torch.cuda.empty_cache()\n        \n    torch.cuda.empty_cache()\n\n    # --- Final Plots ---\n    # Plot losses\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(generator_losses, label=\"Generator\", color='blue')\n    plt.plot(discriminator_losses, label=\"Discriminator\", color='red')\n    plt.plot(diffusion_losses, label=\"Diffusion\", color='green')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Losses\")\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot metrics\n    plt.subplot(1, 2, 2)\n    plt.plot(psnr_scores, label=\"PSNR\", color='orange')\n    plt.plot([s * 20 for s in ssim_scores], label=\"SSIM × 20\", color='purple')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Score\")\n    plt.title(\"Image Quality Metrics\")\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(\"results/training_plots.png\")\n    plt.show()\n    \n    return generator, discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:40:08.978022Z","iopub.execute_input":"2025-05-04T17:40:08.978315Z","iopub.status.idle":"2025-05-04T17:40:09.027241Z","shell.execute_reply.started":"2025-05-04T17:40:08.978294Z","shell.execute_reply":"2025-05-04T17:40:09.026546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, disc = train(\n    epochs=150,          \n    batch_size=8,          \n    lr=0.0001,             \n    diffusion_steps=100,   \n    diffusion_weight=0.2,  \n    save_interval=10        \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:40:24.054727Z","iopub.execute_input":"2025-05-04T17:40:24.055296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:32:20.286000Z","iopub.status.idle":"2025-05-04T17:32:20.286282Z","shell.execute_reply.started":"2025-05-04T17:32:20.286141Z","shell.execute_reply":"2025-05-04T17:32:20.286152Z"}},"outputs":[],"execution_count":null}]}